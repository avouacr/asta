---
title: "module6"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{module6}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


```{r setup}
library(asta)
library(skimr)
library(tidymodels)
library(ggthemes)
```



# menuItem 1 : Classification supervisée

## menuSubItem : données

Choisir la base de données et le pourcentage de test :

```{r}
# Grandile
# data <- grandile   #changer la base de données ("vins","grandile")
# data <- data %>% rename(target = PAUVRE) %>%
#   select(-starts_with("LIB")) %>%
#   select(-IDENT) %>%
#   mutate(target = as.factor(target))

#vins
data <- vins %>% rename(target = quality)
```


En affichage : 
le skim du train (fichier d'entraînement)
le skim du test (fichier test)

En variable globale, on a les bases de données train et test qui seront 
transmises aux sous-modules suivants.

```{r}
#paramètres



part_training <- 3/4 #proportion du découpage aléatoire en training et test
var_strata <- "target" #variable de stratification : souvent la variable target
# nb_folds <-10 #nombre de classes pour la validation croisée

# set.seed(123)
#Découpage en training et en test (ancien découpage)
# data_split <- rsample::initial_split(data,
                                     # strata = .data[[var_strata]],
                                     # prop = part_training)
data_split <- initial_validation_split(data,strata = .data[[var_strata]]) 

train_data <- training(data_split) #le fichier d'entraînement
test_data <- testing(data_split) #le fichier de test
valid_data <- validation(data_split)
train_valid_data <- train_data %>% bind_rows(valid_data)

skim(train_valid_data)
skim(test_data)

```



## menuSubItem : preparation de la base

On part du fichier d'entraînement qui est une variable globale du module précédent.

### Modifs de la base d'entraînement

Une fenêtre avec la modif de la base de données : 
A la fin du module de préparation, la base de données a été modifiée : 
- selection des variables rentrant dans le modèle
- centrage-réduction d'une sélection de variables numériques
- pour transformer des variables nominales en indicatrices
- pour mettre certaines variables au carré ou en interaction
On obtient donc une recette après sélection des différents items et un clic. 


```{r}
#################### 1 - Recettes #####################################----------------

#1 - Création de la recette : on met toutes les variables dans un premier temps
#Voulez vous retirer des variables du modèle ? (liste des variables)
#Voulez-vous centrer réduire les variables ? (oui/non)

#Modèle avec toutes les variables
rec1 <- 
  recipe(target ~ ., data = train_valid_data) 

#Modèle avec deux variables en moins
rec2 <- 
  recipe(target ~ ., data = train_valid_data) %>% 
  step_rm(c(fixed.acidity,volatile.acidity))

#Modèle avec toutes les variables centrées réduites
rec3 <- 
  recipe(target ~ ., data = train_valid_data) %>%
  step_normalize(all_numeric_predictors()) 
# %>% step_dummy(all_nominal_predictors()) %>% #: pour transformer les variables nominales en indicatrices
# %>% update_role(flight, time_hour, new_role = "ID") %>% #: pour retirer des variables du modèle
# %>%step_normalize(all_numeric_predictors()) #pour centrer réduire
# %>% step_zv() #pour enlever les variables avec une seules valeur
# %>% step_rm() #removes variables
# %>% step_impute_mode() #imputation des valeurs manquantes avec le mode
# %>% step_impute_mean() #imputation des valeurs manquates avec la moyenne
# %>% step_clean_names #nettoyer le nom des variables

#Recette pour grandile
rec4 <- 
  recipe(target ~ ., data = train_valid_data) %>%
  # step_rm(all_nominal_predictors())   %>% 
  step_normalize(all_numeric_predictors()) %>%
  # step_string2factor(target) %>% 
  # step_rm(IDENT) %>% 
  step_dummy(all_nominal_predictors()) 

#la recette choisie parmi les recettes ci-dessous quand on clique sur validé
rec <- rec4 #paramètre à changer pour changer de recette
rm(rec1,rec2,rec3,rec4)#pour nettoyer l'environnement
```

Affichage de la table tranformée après la recette

```{r}
data_rec <- bake(prep(rec),new_data = NULL)
skim(data_rec)
```


### Choix du modèle

Paramètre algo : 
Une autre fenêtre avec le choix de l'algorithme : 
- la regression logistique
- l'arbre (hyparamètres par défaut)
- le KNN (hyparamètres par défaut)
- la forêt (hyparamètres par défaut)
On obtient un modèle (à voir si on optimise les hyper-paramètres)

```{r}

##############         2 - Modèles      ########################-----------------

#2 - Choix de l'algorithme :

#2-1 la regression logistique
mod_lr <- 
  logistic_reg() %>% 
  set_engine("glm")

#2-2 la forêt aléatoire
#Les paramètres de la forêt aléatoire : 
# trees nombre d'arbres trees
# Nombre de variables pour chaque arbre
mod_rf <- 
  rand_forest(trees = 1000,
              mtry = 3,
              min_n = NULL) %>% 
  set_engine("ranger") %>% 
  set_mode("classification")


#3-3 l'arbre de décision

mod_tree <- 
  mod_tree <- 
  decision_tree(
    cost_complexity = 0.001,
    tree_depth = 7,
    min_n = NULL
  ) %>%
  set_engine("rpart") %>%
  set_mode("classification")


# mod_tree <- 
#   decision_tree(
#     cost_complexity = tune(),
#     tree_depth = tune(),
#     min_n = NULL
#   ) %>% 
#   set_engine("rpart") %>% 
#   set_mode("classification")
# #Hyper-paramètres à tester, généré automatiquement par grid regular
# grid_tree <- grid_regular(cost_complexity(),
#                           tree_depth(),
#                           levels = 5)
#Je teste 25 combinaisons d'hyper paramètres pour trouver le meilleur arbre

#3-4 KNN
mod_knn <- 
  nearest_neighbor(
    neighbors = 3
  ) %>% 
  set_engine("kknn") %>% 
  set_mode("classification")


#3-5 la regression lasso
mod_lasso <- 
  logistic_reg(penalty = 0.001, 
               mixture = 1) %>% 
  set_engine("glmnet")
#lasso grid : 30 hyper-paramètres à tester
grid_lasso <- tibble(penalty = 10^seq(-4, -1, length.out = 30))

#3-4 la regression ridge
mod_ridge <- 
  logistic_reg(penalty = tune(), 
               mixture = 0) %>% 
  set_engine("glmnet")
#ridge grid : 30 hyper-paramètres à tester
grid_ridge <- tibble(penalty = 10^seq(-4, -1, length.out = 30))

#SVM : machine à support de vecteur
mod_svm <- svm_rbf(mode = "classification", 
                     cost = 10, 
                     rbf_sigma = 0.1, 
                     margin = 1) %>%
  set_engine("kernlab")

#On choisit la recette
mod <- mod_tree #paramètre à changer pour changer de modèle

rm(mod_lr,mod_ridge,mod_lasso,mod_rf,mod_tree,mod_knn,mod_svm)
```


```{r}
############# 3 - Workflows     ########################--------------
#3 - Création des workflows

#Workflow sans tuning
wflow <-  workflow() %>% 
  add_model(mod) %>% #ajout de la regression logistique
  add_recipe(rec)
mod
wflow
```

### Validation

On joint la recette et le modèle pour arriver à un workflow, qui est la 
variable globale de ce module qu'on va utiliser dans le sous-module suivant. 

Dans ce module, on affiche la base d'entraînement (qui peut être modifiée) et le modèle choisi + son nom.

On ne fait que la validation croisée sur les données d'entraînement. On peut changer le nombre de folds.
On choisit un workflow dans la liste (avec une recette et un modèle) et on affiche les résultats.
ça peut prendre plus ou moins de temps selon l'alogorithme choisi. 

```{r}
##############Validation croisée##################
# set.seed(345)
nb_folds <- 5
folds <- vfold_cv(train_valid_data,
                  v = nb_folds)#paramétrage de la validation croisée
# 
# Evaluation du modèle avec la validation croisée
metrics <- metric_set(accuracy,recall, precision,roc_auc,sensitivity, specificity)
fit_rs <- wflow %>% fit_resamples(folds,
                                  metrics = metrics,
                                  control = control_resamples(save_pred = TRUE))

nb_rs_metrics <- collect_metrics(fit_rs)
nb_rs_predictions <- collect_predictions(fit_rs)

fit_rs %>% collect_metrics()
conf_mat_resampled(fit_rs, tidy = FALSE) %>%
  autoplot(type = "heatmap")
  

```


```{r}
#############validation simple##############

fit <- wflow %>% fit(train_data)

predict(fit,new_data = valid_data)
pred_valid <- augment(fit, valid_data)

roc_plot_valid <- pred_valid %>% 
  roc_curve(truth = target, .pred_1) %>% 
  autoplot()
roc_plot_valid

#Aire sous la courbe
pred_valid %>% 
  roc_auc(truth = target, .pred_1)

#Accuracy : pourcentage de biens classés
pred_valid %>% 
  accuracy(truth = target, .pred_class)

#Spécificité
pred_valid %>% 
  specificity(truth = target, .pred_class)

#Sensitivité
pred_valid %>% 
  sensitivity(truth = target, .pred_class)

pred_valid %>%
  conf_mat(target, .pred_class) %>%
  autoplot(type="heatmap")
  
  # pluck(1) %>%
  # as_tibble() %>%
  # ggplot(aes(Prediction, Truth, fill = n)) +
  # geom_tile(show.legend = FALSE) +
  # geom_text(aes(label = n), colour = "white", alpha = 1, size = 5) +
  # ggtitle("Table de confusion") +
  # xlab("Classe prédite") + ylab("Classe réelle") +
  # theme_hc() + scale_colour_hc()
```


A la fin de cette étape, on a choisi un modèle : on prend le meilleur au vu des résultats et on clique sur OK. 
c'est ce modèle qui sera utilisé pour la généralisation sur la phase de test. 

## menuSubItem : généralisation

En entrée, on a le modèle qui a été sélectionné.

```{r}
#5 - Visualisation du résultat (regression logistique et random forest)

fit_final <- 
  wflow %>% 
  fit(data = train_valid_data) 
  
#6 - Prédiction sur la base de test
pred_testing <- augment(fit_final, test_data) #renvoie une base avec aussi les probas 


#affichage de la courbe ROC
roc_plot_testing <- pred_testing %>% 
  roc_curve(truth = target, .pred_bon) %>% 
  autoplot()
roc_plot_testing

#Aire sous la courbe
pred_testing %>% 
  roc_auc(truth = target, .pred_bon)

#Accuracy : pourcentage de biens classés
pred_testing %>% 
  accuracy(truth = target, .pred_class)

#Spécificité
pred_testing %>% 
  specificity(truth = target, .pred_class)

#Sensitivité
pred_testing %>% 
  sensitivity(truth = target, .pred_class)

pred_testing %>%
  conf_mat(target, .pred_class) %>%
  pluck(1) %>%
  as_tibble() %>%
  ggplot(aes(Prediction, Truth, fill = n)) +
  geom_tile(show.legend = FALSE) +
  geom_text(aes(label = n), colour = "white", alpha = 1, size = 5) +
  ggtitle("Table de confusion") +
  xlab("Classe prédite") + ylab("Classe réelle") +
  theme_hc() + scale_colour_hc()

```


On a sélectionné le meilleur modèle et on teste ses performances sur des données qu'il n'a jamais vu. 
On fait apparaître le tableau de truc + courve AUC + Accuracy (pour voir le taux de biens classés par exemple).

# menuItem 2 : Regression

## Données


```{r}
data <- departements %>% rename(target = nb_habitants) 

# data <- mtcars %>% rename(target = mpg)
```

Séparation de la base

```{r}

data_split <- initial_validation_split(data) 

train_data <- training(data_split) #le fichier d'entraînement
test_data <- testing(data_split) #le fichier de test
valid_data <- validation(data_split)
train_valid_data <- train_data %>% bind_rows(valid_data)

skim(train_valid_data)
skim(test_data) 
```


## Préparation

```{r}
rec1 <- 
  recipe(target ~ ., data = train_valid_data) %>% 
  step_dummy(GR_REG)

rec <- rec1
```

```{r}
prep <- prep(rec)
bake <- bake(prep, new_data = NULL)
bake
```


## Modèles

```{r}

#modèle SVM
mod_svm <- svm_linear() %>%
  set_mode("regression") %>%
  set_engine("LiblineaR")

#random forest
mod_rf <- rand_forest(trees = 1000) %>%
  set_engine("ranger") %>%
  set_mode("regression")

mod_lr <- linear_reg() %>% 
  set_engine("lm") 


mod <- mod_rf
rm(mod_lr,mod_rf,mod_svm)
```


```{r}
wflow <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(mod)
wflow
```

## Validation

### Validation croisée

```{r}
# set.seed(345)
nb_folds <- 5
folds <- vfold_cv(train_valid_data,
                  v = nb_folds)#paramétrage de la validation croisée
# 
# Evaluation du modèle avec la validation croisée
metrics <- metric_set(rmse,rsq,mae, mape)
fit_rs <- wflow %>% fit_resamples(folds,
                                  metrics = metrics,
                                  control = control_resamples(save_pred = TRUE))

nb_rs_metrics <- collect_metrics(fit_rs)
nb_rs_predictions <- collect_predictions(fit_rs)


nb_rs_metrics

```

### Validation simple

## Généralisation
